---
title: "Algorithms Programming PS1"
author: "Josh Kaplan & John Hotchkiss"
date: "3/2/2017"
output:
  pdf_document: default
---

# Algorithm Choice

To begin our search for an algorithm for this assignment, we decided it would be safest to stick to those that we learned in class.  That meant we needed to decide between Kruskal's and Prim's algorithms.  After considering our abilities (we could only reasonably expect to use a binary heap for Prim's), we looked at the complexities for each algorithm.  In particular, Prim's with a binary heap runs in $O\left(\left(|E|+|V|\right)\log|V|\right)=O\left(|E|\log|V|\right)$ time.  Kruskal's runs in $O\left(|E|\log|V|^2\right)=O\left(2*|E|\log|V|\right)=O\left(|E|\log|V|\right)$ time.  Clearly, then, Prim's and Kruskal's are asymptotically the same given our intended implementation strategies.  As such, we decided to pursue Prim's algorithm because we felt more comfortable with a binary heap than we did with a UnionFind data structure.

# Part 1
```{r, include=FALSE}
# d = 0
x = c(16,32,64,128,256,512,1024,2048,4096,8192,16384)
y0 = c(1.1761516976976152, 1.2893791594683865, 1.2011415156545038, 1.2625970167532716, 1.2508610725981701, 1.1821490332236186, 1.2538311212322837,1.2049225953543985, 1.2026818013521898, 1.2112227573940526, 1.1987999451103029)

# d = 2
y2 = c(2.859971752205042, 3.741835235070977, 5.489154295536831, 7.496303082807269, 10.619540670775686, 14.964802591719781, 21.040327443505742, 29.607739776945582, 41.82266382000974, 58.884759462822345, 83.18002071228824)

# d = 3
y3 = c(4.659840488760341, 7.133291529472253, 11.590927872145325, 17.625159531916225, 27.760603800328834, 43.247524935394786, 68.4321011345506, 107.20985631396886, 169.21941242331545, 267.04187464717745, 423.11686250212784)

# d = 4
y4 = c(6.156334571526998, 10.577551767646812, 16.963940428354896, 28.485926374928717, 47.04827597259361, 78.28180749387901, 130.19475236633826, 216.600937504728, 360.83195135541087, 603.6963175409591, 1008.8890848383464)
```

```{r, echo=FALSE, out.width = '500px',out.height='500px', dpi=200}
par(mfrow=c(1,2))
plot(x,y0,ylim=c(0,1.4),ylab = 'average tree size', xlab = 'nodes',
     main = 'Average tree size, 5 runs, d=0')
curve(.75*atan(x/20)+.02,from=2, to =16000,col='red',add=TRUE)

plot(x,y2,ylab = 'average tree size', xlab = 'nodes',
     main = 'Average tree size, 5 runs, d=2')
model2 = lm(y2~poly(log(x),4,raw=TRUE))
test = seq(16,16000,1)
vals= predict(model2,list(x = test))
lines(test,vals,col='red')
```

For $d=0$, the tree size basically stops increasing after about 32 nodes, settling down to about 1.2. We tested smaller graph sizes than 16 in order to fit the curve, and found that arctan works particularly well because it also settles down very quickly (An analysis of applying arctangent follows in Part 2). For $d=0$, 

$$ f(n) = .75*\arctan(n/20)+.02$$

$d=2$ clearly exhibits different behavior - the tree size increases with the nodes at a decreasing rate, characteristic of a logarithmic function. We used regression to find a function that exactly fits our data points (The $R^2$ was $>.999$). The result was: 

$$ f(n) = `r model2$coefficients[1]` +`r model2$coefficients[2]`*\log(x) +  `r model2$coefficients[3]`*\log(x)^2 + `r model2$coefficients[4]`*\log(x)^3 +  `r model2$coefficients[5]`*\log(x)^4 $$

The y-intercept is not very meaningful in this cases; it is set so that the model fits, but it doesn't make a lot of sense because the model is not based on any data from $n=0-15$.   

```{r, echo=FALSE}
par(mfrow=c(1,2))
plot(x,y3,ylab = 'average tree size', xlab = 'nodes',
     main = 'Average tree size, 5 runs, d=3')
model3 = lm(y3~poly(log(x),4,raw=TRUE))
test = seq(16,16000,1)
vals= predict(model3,list(x = test))
lines(test,vals,col='red')

plot(x,y4,ylab = 'average tree size', xlab = 'nodes',
     main = 'Average tree size, 5 runs, d=4')
model4 = lm(y4~poly(log(x),4,raw=TRUE))
test = seq(16,16000,1)
vals= predict(model4,list(x = test))
lines(test,vals,col='red')
```

Again, we used regression to find functions that exactly fit our data points. The functional form for d=3,4 was basically identical to d=2, only the model coefficients changed. For d=3,

$$ f(n) = `r model3$coefficients[1]` +`r model3$coefficients[2]`*\log(x) +  `r model3$coefficients[3]`*\log(x)^2 + `r model3$coefficients[4]`*\log(x)^3 +  `r model3$coefficients[5]`*\log(x)^4 $$

and for d=4,

$$f(n) = `r model4$coefficients[1]` +`r model4$coefficients[2]`*\log(x) +  `r model4$coefficients[3]`*\log(x)^2 + `r model4$coefficients[4]`*\log(x)^3 +  `r model3$coefficients[5]`*\log(x)^4 $$

Again, the y-intercepts are not especially meaningful in these cases because the  model does not include data from $n=0-15$.

# Part 2
## Growth rates of f(n)

For d=2,3, and 4, the logarithmic behavior of f(n) makes sense. As n increases, each node has more edges coming off of it (since the graph is complete), so Prim's algorithm has exponentially more edges to choose from in selecting edges of minimum weight. Thus, you would expect the average edge weight to decrease as n increases. On the other hand, there are more nodes in the graph, so the MST must contain more edges in order to connect them all. So you would expect the average tree size to increase. The nodes (and the number of edges in the tree) increases linearly, while the average edge weight decreases logarithmically. Thus, we see logarithmic growth in the size of the tree. This pattern holds for graphs in any dimension >1, but higher dimensions result in overall higher edge weights, so the average tree size is a constant factor higher for all n.

We were surprised to see the quick convergence of the tree size in d=0. After about 30 nodes, the tree doesn't grow no matter how many nodes are added to the graph. After some internet research, we discovered that the limit on the weight of the spanning tree, as n goes to infinity, is equal to Riemann zeta(3) $\approx 1.2$.

## General Implementation Strategy/Experience

Always, we simply tried to implement aspects of the algorithm in the most straightforward way possible, but this led to many inefficiencies we eventually needed to remove.  For example, we began by learning how to generate a graph with a list containing all of the vertices and a list containing all of the edges and edgeweight information.  Then, we fed these things to our implementation of Prim's.  Because edges were a simple list (node1,node2,edgeweight), which worked fine for graph representation, we had to expend a lot of runtime and effort extracting the information back out of that data format in Prim's.

In particular, consider one of the deeper lines of pseudo code for Prim's: for $(v,w)\in E$ and $w \notin S^\prime$.  This required looping through all of our edges, $E$, extracting the node information, checking to see if the source node, $v$, was either of the nodes associated with the edge, $u$ OR $w$, then assigning $w$ to whichever the source was not, all before continuing to the next line of the algorithm.

This issue stemmed from not approaching the task more holistically.  Many efficiencies were gained when we stopped treating the tasks (generating a graph and creating a minimum spanning tree) as two separate tasks and instead one larger task.

Finally, our implementation made clear that the devil is in the details.  It is one thing to consider an algorithm as pseudocode and entirely another to make it functional.  For example, we needed to change our heap so that it would notice whether or not a node that was trying to be inserted was already on the heap, and if so, would change the value of that node instead of adding a duplicate.  This required adding a dictionary to the internals of the heap that would keep track of the index of particular nodes within the heap.

## Optimization Explanations

We made a few optimizations.  Some of them were quite small and probably made little difference.  However, the two largest optimizations are below:

1. Following the hint in the assignment, we attempted to find a weight threshold as a function of $n$, $k(n)$.
2. Storing edges in a dictionary format.  This stores all of the edges that we keep twice, once for each node belonging to the edge.  Entries in the dictionary are (node, edgeweight).


### Determining edge weight threshold k(n)

To find a threshold function, $k(n)$, we first gathered data from the algorithm.  We collected the maximum edge weight over 5 runs for $n$ as a power of 2 at the same levels as those we used for the other graphs.  After putting those values over a scatter plot, we fit a curve to them using the strategy described below.

We chose to model the curve using a tangent function due to its desirable mathematical properties.  In particular, we thought it would be helpful to have a curve with built in asymptotes.  The asymptotic properties can help to mitigate any odd behavior at extremely large values of $n$ that could occur in equations without such properties (such as an eventual droop to a negative threshold or a threshold equal to zero). Recall an arctangent curve without modification,

```{r, echo=FALSE,fig.show="hold",fig.keep="last", out.width = '350px',out.height='350px',fig.align='center'}
plot(curve(atan(x), from=-45, to=45),xlim=c(-40,40),
     type='l',xlab="X",ylab="y",main="Vanilla Arctangent(x)")
abline(v=0)
abline(h=0)
abline(h=pi/2, lty=2)
abline(h=-pi/2, lty=2)
```

Note the asymptotes at $\frac{\pi}{2} \text{ and } -\frac{\pi}{2}$. By negating $x$ we can flip the function over the x-axis.  Then, by adding $\frac{\pi}{2}$ the bottom asymptote will lie on $y=0$.  So for 

$$
\text{arctan}(-x)+\dfrac{\pi}{2},
$$
we have:

```{r, echo=FALSE,fig.show="hold",fig.keep="last", out.width = '350px',out.height='350px',fig.align='center'}
plot(curve(atan(-x)+pi/2, from=-45, to=45),xlim=c(-40,40),
     type='l',xlab="X",ylab="y",main="Arctangent(-x) + pi/2")
abline(v=0)
abline(h=0, lty=2)
abline(h=pi, lty=2)
```

Now consider that by dividing $x$ we can make the slope less steep.  Furthermore, by multiplying the entire equation by a value we can change the height of the bend (and increase the $y$-intercept) without moving the asymptote at $y=0$.  Consider

$$
\dfrac{4}{\pi}\left(\text{arctan}\left(-\dfrac{x}{2}\right)+\dfrac{\pi}{2}\right).
$$

```{r, echo=FALSE,fig.show="hold",fig.keep="last", out.width = '350px',out.height='350px',fig.align='center'}
plot(curve(4/pi*(atan(-x/3)+pi/2), from=-45, to=45),xlim=c(-40,40),
     type='l',xlab="X",ylab="y",main="(4/pi)*(Arctangent(-x/2) + pi/2)")
abline(v=0)
abline(h=0, lty=2)
abline(h=4, lty=2)
```

Using the sorts of manipulations described above, we could modify the tangent curves to fit the plots, as you can see below for each dimension:

Dimension 0:

$$
\dfrac{6}{\pi}\left(\text{arctan}\left(-\dfrac{x}{20}\right)+\dfrac{\pi}{2}\right)
$$

Dimension 2:

$$
\dfrac{3}{\pi}\left(\text{arctan}\left(-\dfrac{x}{35}\right)+\dfrac{\pi}{2}\right)+0.03
$$

Dimension 3:

$$
\dfrac{10}{\pi}\left(\text{arctan}\left(-\dfrac{x}{30}\right)+\dfrac{\pi}{2}\right)+0.08
$$

Dimension 4:

$$
\dfrac{5}{\pi}\left(\text{arctan}\left(-\dfrac{x}{90}\right)+\dfrac{\pi}{2}\right)+0.16
$$

```{r,echo=FALSE}
y0 = c(0.3400149310355898, 0.1369654371267507, 0.0766013703918006, 0.05166156780009534, 0.03450585230327807, 0.01700032980212829, 0.007636573427697746, 0.004752657563726892, 0.002843524893660798, 0.0012263764848782222, 0.0007029643572269428)
y2 = c(0.49700727995966326, 0.2980751627287141, 0.2540554938495211, 0.17405003474016104, 0.11401667263363004, 0.08568559433081759, 0.06438714250452654, 0.04634966943492177, 0.03172059598839271, 0.026590282883345022, 0.019232107099199234)
y3 = c(0.6330031148843387, 0.5498232748051404, 0.4150841270269425, 0.30871665669525744, 0.2374499083701159, 0.18478302674141103, 0.14379926202304796, 0.12771658182029552, 0.11455778759673739, 0.08361918318479254, 0.06753415145190123)
y4 = c(0.6813856548833109,0.5673670123978358,0.5279571807152447,0.5284341610145022,0.3718192072923492,0.2911852878506642,0.3033865546337661,0.2062859629548089,0.19876949772880462,0.1800504851416741,0.13303175233407574)

par(mfrow=c(1,2))
plot(x,y0, xlab = 'nodes',ylab = 'edge weight', main = 'Max edge size, d=0')
curve((3/pi)*(atan(-x/20)+pi/2)*2,from=2, to =16000,col='red',add=TRUE)

plot(x,y2, xlab = 'nodes',ylab = 'edge weight', main = 'Max edge size, d=2')
curve((3/pi)*(atan(-x/35)+pi/2)+0.03,from=2, to =16000, add=TRUE,col='red')

plot(x,y3, xlab = 'nodes',ylab = 'edge weight', main = 'Max edge size, d=3')
curve((10/pi)*(atan(-x/30)+pi/2)+0.08,from=2, to =16000, add=TRUE,col='red')

plot(x,y4, xlab = 'nodes',ylab = 'edge weight', main = 'Max edge size, d=4')
curve((5/pi)*(atan(-x/90)+pi/2)+0.16,from=2, to =16000, add=TRUE,col='red')
```

In the event that the pruned graph is too pruned, our algorithm will call itself with a higher threshold (previous threshold + 0.01) and try again.  This was a necessary addition because it is possible to remove too many edges.  In the event that too many edges are removed, Prim's algorithm will be unable to create a spanning tree, and will stop before all nodes have been reached (there will be nodes with minimum distances still equivalent to infinity).  It does not, however, fail by creating the wrong tree.  This is because if there is not an edge to connect the node to the tree, it is because they were all thrown out.  If there _is_ an edge or multiple edges, they must be the shortest edges that connect to that node, in which will be included the shortest.  It is not possible to keep a longer edge than the shortest, because by definition the shortest would have been kept also.

### Applying a Dictionary to Store Edges

Another big improvement in runtime came when we stopped having to loop over all edges in order to find those that were involved in a particular node, $v$.  This applies to the line referenced earlier from the pseudo code, for $(v,w)\in E$ and $w \notin S^\prime$, and is a response to looking at the task at hand more holistically.  Instead of restricting ourselves to a full list of edges, we simply save edges from the time of generation into a dictionary and only if they pass the threshold.    In this way, we never actually have the complete list of edges stored.  For the dictionary, keys are nodes.  Values for the dictionary are the partner node and the weight of the edge they share.  That means each edge appears twice in the dictionary (once for each node).

This and other small changes changed our ``for edges"" loop from

```{}
for e in E:
    if d!=0:
        enodes = [tuple(i) for i in e[0]]
    else:
        enodes = e[0]
    if v[0] in enodes:
        w = enodes[1-enodes.index(v[0])]
        if w in SP:
            if distd[w] > e[1]:
                distd[w] = e[1]
                prevd[w] = v[0]
                H.insert((w,distd[w]))
```

to

```{}
for e in E[v[0]]:
    if e[0] in SP:
        if distd[e[0]] > e[1]:
            distd[e[0]] = e[1]
            H.insert((e[0],distd[e[0]]))
```

## Runtime

```{r,echo=FALSE}
y0 = c(0.001385000000000025, 0.004056000000000004, 0.006791000000000047, 0.012427000000000021, 0.028332000000000024, 0.07329400000000003, 0.22835999999999995, 0.79093, 2.624345, 9.853193000000001, 38.410444)
y2 = c(0.0010130000000003747, 0.0030989999999988527, 0.008869999999994604, 0.025455000000000894, 0.08362499999999784, 1.0456080000000014, 1.334620000000001, 5.201846000000003, 20.010974000000004, 82.798501, 327.699589)
y3 = c(0.0026419999999802712, 0.005560000000002674, 0.0105829999999969, 0.03440499999999247, 0.11828499999995756, 0.39876499999996895, 1.6079859999999826, 6.416526000000033, 25.43127800000002, 101.78053699999998, 413.8965300000001)
y4 = c(0.002410000000054424, 0.0047559999998156854, 0.014128999999911684, 0.039420999999947526, 0.12845999999990454, 0.4249720000000252, 1.6997339999998076, 6.839534999999842, 27.931145000000015, 109.77722000000017, 489.96768799999995)

par(mfrow=c(1,2))
plot(x,y0,xlab = 'nodes',ylab = 'runtime (seconds)', main = 'runtime, d=0')
plot(x,y2,xlab = 'nodes',ylab = 'runtime (seconds)', main = 'runtime, d=2')
plot(x,y3,xlab = 'nodes',ylab = 'runtime (seconds)', main = 'runtime, d=3')
plot(x,y4,xlab = 'nodes',ylab = 'runtime (seconds)', main = 'runtime, d=4')
```

For all choices of d, runtime exhibits the same functional form with respect to the number of nodes. This is the characteristic O(n log n) curve, which makes sense as, due to our optimization reducing the number of edges submitted to Prim's algorithm, |E| is O(n). The runtime increases substantially with d, as well; a line-by-line analysis indicates that much of the increased time comes in the graph generation stage - calculating the Euclidean distance between nodes in 4 dimensions is substantially more costly than 2 dimensions. For all d and large n, nearly all of the runtime depicted above comes from   
generating the random graphs - for n = 16000, d = 0, Prim's algorithm takes 6 seconds to run on 305343 edges which met the threshold, while generating those edges takes 925 seconds.